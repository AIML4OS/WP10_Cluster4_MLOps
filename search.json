[
  {
    "objectID": "chapters/chapter3.html",
    "href": "chapters/chapter3.html",
    "title": "Serving",
    "section": "",
    "text": "Model serving (API)\n\n\nDeployment\n\n\nFrom dev to production: deal with environments",
    "crumbs": [
      "Serving"
    ]
  },
  {
    "objectID": "chapters/chapter1.html",
    "href": "chapters/chapter1.html",
    "title": "Data",
    "section": "",
    "text": "One of the most important practices for Machine Learning projects is to strictly separate data, code (incl. model architecture, training code, API etc.) and the compute environment.\nEnforcing such a separation enable to:\n\nhave a strict reproducibility of the full pipeline\nindependence and better maintainability for each of the components\n\n\nData storage\nIn that spirit, data should absolutely lie in a stable storage - preferably cloud-based, far from the messy environment of code and compute. If your code or your computer crashes, your data should be safe.\nBeyond where data is stored, how it is stored matters just as much. For text classification pipelines, columnar formats such as Parquet are generally recommended over raw CSV or JSON files. Parquet is compressed, schema-aware, and optimized for analytical workloads, which means faster reads, lower storage costs, and more predictable behavior across tools. In practice, this allows you to load only the columns you need (e.g. text and labels), enforce consistent data types, and efficiently handle large datasets during training, validation, and monitoring. Parquet is also natively supported by most modern data processing frameworks (Spark, Pandas, Polars, DuckDB), making it a robust and interoperable choice for production-grade ML pipelines.\nAny preprocessing step should be clearly documented, with a fully reproducible script.\n\n\n Insee: S3-based storage\n\n\nAt Insee, we extensively use cloud-based S3 data storage solution, based on the open-source MinIO framework - be it on the SSP Cloud (public Onyxia instance for collaborative, non-sensitive use cases) or LS3 (the internal Onyxia instance for secured, offline projects).\nAccess your data from the storage is then very easy, from any compute environment (think of it as a Google Drive share link for instance).\nFor instance in Python:\n\n\nCode\n# Connecting to the storage via a filesystem\nfs = S3FileSystem(\n        client_kwargs={\"endpoint_url\": f\"https://{os.environ['AWS_S3_ENDPOINT']}\"},\n        key=os.environ[\"AWS_ACCESS_KEY_ID\"],\n        secret=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n    )\n\n# Loading a dataframe is very easy !\ndf_train = pd.read_parquet(\"df_train.parquet\", filesystem=fs)\n\n# Saving too\ndf_train.to_parquet(\"df_train.parquet\", filesystem=fs)\n\n\n\n\n\n\n Destatis:\n\n\nIn order to ensure that the data is stored and used efficiently we make use of the Hadoop Distributed File System (HDFS) and parquet for data partitioning. HDFS is especially made for handling a large amount of data. For programming and data processing, we use Cloudera Machine Learning (CML) with PySpark, which allows us to efficiently work on the data. We store our data in the Parquet format, which is ideal for big data and in addition, to make it easier for users to handle and cross-check the data, we use Hue (Hadoop User Experience), an open-source SQL-based cloud editor. For rights management, we use Ranger, which provides a big variety of access control to ensure data security.\nThe data cleaning in our project is quite straightforward, since the text entries contain short texts (mostly keywords) instead of long ones. First, data augmentation is performed by adding new text entries (e.g. text like “groceries” or “beverages”) to the dataset, adding multiple newly generated text values to each household to enrich the data. Adding a variety of new textual entries helps the model to generelize better. Secondly, we clean the data by removing punctation and handling missing values.\n\n\n\n\n Austria:\n\n\nTraining data is stored as csv-files. New files are added quarterly by the subject matter experts (between 300-500 data entries), which are then used as to retrain the model.\nDuplicated entries are removed from the data. Text inputs are transformed into all lower-case letters. Further, we remove stop words, umlaut-charaters (ä,ö,ü), special characters (e.g. -,+,#,), gender-specific words endings (e.g. “-in”, “:innen”), and numbers. Each categorical variable has a predefined set of valid input classes, since the model can only handle known classes. All known inputs are translated into this set of classes. Unknown inputs are set to their respective “unknown” category.\n\n\n\n\nData versioning\n\n\n Insee: MLFlow Datasets\n\n\nJust as code (see chapter 2), a good practice is to version the dataset, to exactly know on which data the model has been trained (or which is the latest version for the model to be trained on).\nSeveral tools are available to seamlessly achieve this versioning:\n\nMLFlow Datasets\nDVC\n\nStill WIP at Insee.\n\n\n\n\n\n\n\n\nTipFurther reading\n\n\n\n\nThree Levels of ML Software by ml-ops.org\nReproducibility guidelines by Anaconda",
    "crumbs": [
      "Data"
    ]
  },
  {
    "objectID": "chapters/chapter2.html",
    "href": "chapters/chapter2.html",
    "title": "Model",
    "section": "",
    "text": "Model training\n\nParallelized training for maximum reproductibility -&gt; Fix the seed -&gt; Parallelized training with tools such as Argo Workflows -&gt; Logging tools (MLFlow, Weights & Biases…)\n\nModel training at scale with Spark: general lessons learned\nWhen text classification datasets grow beyond the limits of single-machine parallelization in Python or R, distributed frameworks such as Apache Spark become a natural consideration. Spark enables horizontal scaling across clusters and is particularly well suited for large-scale data preparation, feature extraction, and the training of classical machine-learning models on datasets that no longer fit into memory. In an MLOps context, Spark integrates well with cloud storage, columnar formats such as Parquet, and fault-tolerant execution, making it attractive for industrial and institutional environments.\nHowever, experience shows that Spark should be used selectively for model training. While it excels at scaling data processing and simple models, its native machine-learning library (MLlib) offers a limited algorithm portfolio and constrained options for hyperparameter tuning and model optimization. As a result, Spark-based ML pipelines are most effective when applied to well-understood, classical models and when training speed, robustness, and operational scalability are prioritized over modeling flexibility. For modern NLP approaches—particularly those based on transformer architectures—Spark is typically better positioned as a data engineering backbone rather than as the primary training framework.\n\n\n Practical experience: Spark-based text classification at Destatis\n\n\nThese general observations are confirmed by practical experience at Destatis, where Spark was evaluated for large-scale text-to-code classification in the context of COICOP expenditure coding. Several experiments were conducted on both a traditional R-based infrastructure and a Cloudera Spark environment, focusing on performance, scalability, and operational feasibility.\nThe results show that classical models can achieve solid predictive performance across platforms. A Random Forest model trained on an R server delivered strong results (Macro F1 ≈ 0.80, Accuracy ≈ 0.90) but required very high memory consumption (~200 GB) and its hyperparameters could only be fine-tuned on data subsamples, limiting its suitability for regular re-trainings. Logistic regression experiments on the R server using scikit-learn failed to converge or required runtimes that were considered too risky for production use.\nIn contrast, logistic regression implemented with Spark MLlib achieved competitive performance (Macro F1 ≈ 0.78, Accuracy ≈ 0.88) while dramatically reducing training time (3–5 minutes) and keeping memory usage at a manageable level (~40 GB). Even with grid search over multiple parameter combinations, total runtimes remained operationally acceptable. More complex models, such as Random Forests in MLlib, proved unsuitable for this high-cardinality multi-class problem, leading to memory errors and limited model quality.\nOverall, this experience illustrates a key MLOps takeaway: Spark can be highly effective for scalable, robust training of classical text classification models, especially when retraining frequency, runtime stability, and infrastructure constraints matter. At the same time, its limitations in model diversity and optimization flexibility mean that Spark-based ML should be carefully scoped, especially in text classification context.\n\n\n\n\nModel validation\n\nKey metrics to be checked before deployment\nYou should have a fully reproductible validation script/pipeline that seamlessly takes a trained model and output the validation metrics\nBest practice: the validation should be run automatically after training and logged\n\nModel evaluation strategy in our specific Text-To-Code context\nFor countries applying machine learning to large-scale text classification, especially in text-to-code scenarios, model evaluation must be adapted to the specific characteristics of statistical classification systems: many classes, strong class imbalance, and high semantic proximity between categories. In such contexts, overall accuracy alone is rarely sufficient and may even be misleading. Instead, evaluation strategies should emphasize precision, recall, and F1-score, which provide a more meaningful view of model behavior under imbalance.\nFor multi-class problems, the aggregation strategy of metrics is a critical design choice. Macro, micro, and weighted variants of F1-score answer different questions and should be selected based on how classification errors are valued. If errors on small or rare classes are substantively important, unweighted macro metrics are preferable, as they prevent dominant classes from masking poor performance elsewhere. Conversely, weighted metrics may be appropriate when the operational focus is primarily on high-frequency categories. In all cases, relying solely on aggregated metrics is insufficient; class-level evaluation is essential to identify systematic biases and uneven model performance.\ndataset stratification is an important design choice rather than a universal requirement. Stratified train–test splits by class are particularly valuable when poor model performance on rare categories is a critical concern, as they ensure that small but substantively relevant classes are adequately represented during evaluation. However, if the primary objective is to optimize performance on high-frequency classes and errors on rare classes are considered less critical, strict stratification may be less essential. The choice should therefore be guided by how classification errors are weighted and interpreted in the specific application context.\nFinally, evaluation datasets must be representative of future data, not just historical samples. Regular evaluation on newly collected data, combined with expert review, is necessary to detect model drift and to ensure sustainable model quality over time.\n\n\n Practical experience: evaluation strategy at Destatis\n\n\nThese principles were applied in practice at Destatis in the context of large-scale COICOP text-to-code classification. Given the highly imbalanced class distribution and the presence of several hundred target classes, the evaluation strategy deliberately moved beyond accuracy as a primary metric. While accuracy was still reported, the main focus was placed on precision, recall, and F1-score, which better capture performance differences across classes of varying frequency.\nIn particular, unweighted macro F1-score was chosen as a key benchmark metric. This choice reflects the statistical objective of treating all COICOP categories as equally important, regardless of their frequency in the data, and of avoiding systematic neglect of rare but substantively relevant classes. Weighted metrics were considered but deemed less suitable, as they tend to understate errors on small classes and can mask biased prediction patterns—for example, consistently favoring dominant categories over closely related but less frequent ones.\nTo support robust evaluation, all train–test splits were stratified by COICOP class, ensuring that both frequent and rare categories were adequately represented in the test data. In addition to aggregated metrics, class-level performance indicators were systematically analyzed to identify weak classes and guide targeted improvements, such as enriching training data for problematic categories.\nFinally, evaluation at Destatis extended beyond static test sets. To measure the robustness of the different models and to ensure relevance for future production use, model outputs were compared with classifications produced by domain experts. This concerns records where the model scores used in production were below a fixed thresholds. This continuous expert-based validation targeting difficult records ensures that the model remains aligned with evolving data and classification practices and naturally forms the basis for a human-in-the-loop framework, which is discussed in the following section.\n\n\n\n\nModel wrapping\n\nEncapsulates a trained and validated model for easy service\nWhile the model per se takes preprocessed/tokenized tensors as input, the wrapper aims at taking RAW text and outputting readable predictions (not logits)\n\nA single .predict() method should work seamlessly\n\nHandle all the preprocessing steps (and the internal machinery needed to run inference)\nThe package torchTextClassifiers has been developed in this mindset\nMLFlow is also naturally designed to help you do that\n\n\n\n Practical experience: model wrapping at Destatis\n\n\n\nmodel functionality of CML is used to deploy a model\nYou can deploy automated pipelines of analytics workloads in different programming languages (R, Python, etc.)\nIn addition you can train, evaluate models and deploy models as REST APIs to serve predictions\neasy to use through a user defined function (e.g. predict()) which generates the predictions in json-Format\nEXAMPLE CODE VANILLA FUNCTION\nsimplified deployment limits flexibility e.g.\n\nCustom deployment scripts\nAdvanced model diagnostics and performance tracking options (like in mlflow)\n\n\n\n\n\n\nModel storage & versioning\n\nYou should keep track of all the experiments (all the architectures, all the different hyperparameters), and you should be able to load any experiment that you have tried, at any time\nThe logging tools generally also handles the storage part\nTo “promote” a model once you are satisfied with its performance (and make it ready for deployment), you should have a way to tag and version your models (ex: SVM-v2, BERT-v4…) and so on.\n\nAt deployment time, you should be able to fetch a model only using its tag and its version (including a previous one if something suddenly broke !)\n\n\nAt Insee : MLFlow\n\n\n Practical experience: model storage & versioning at Destatis\n\n\n\nCML offers model registry so models can be tracked and a roll back to previous model versions is possible\nlike model wrapping, the model storage is simpliefied",
    "crumbs": [
      "Model"
    ]
  },
  {
    "objectID": "chapters/chapter4.html",
    "href": "chapters/chapter4.html",
    "title": "Monitoring",
    "section": "",
    "text": "Observability\n\n\nRetraining",
    "crumbs": [
      "Monitoring"
    ]
  }
]