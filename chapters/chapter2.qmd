---
title: "Model"
format: html # This qmd file will be compiled as an HTML web page
---

# Model training

- Parallelized training for maximum reproductibility
-> Fix the seed
-> Parallelized training with tools such as Argo Workflows
-> Logging tools (MLFlow, Weights & Biases...)

<details class="austria">
<summary class="austria-header">
  <span class="solutionbox-icon"></span>
  Austria: 
</summary>

<div class="solutionbox-body">

Due to the small volume of training data, parallelized training is not required. Instead, we use Keras and TensorFlow in R via reticulate to train our models. Training logs are stored for subsequent analysis.

Hyperparameter tuning is not performed every time the model is retrained; instead, we reuse the hyperparameter configuration that was identified as optimal during the initial model training.

</div>
</details>



# Model validation

- Key metrics to be checked before deployment
- You should have a fully reproductible validation script/pipeline that seamlessly takes a trained model and output the validation metrics
- Best practice: the validation should be run automatically after training and logged


<details class="austria">
<summary class="austria-header">
  <span class="solutionbox-icon"></span>
  Austria: 
</summary>

<div class="solutionbox-body">

Before the model is deployed, several validation steps are performed. These include the calculation of evaluation metrics and a comparison with the previous model version. Evaluation metrics are automatically generated each time the model is retrained. Metrics such as accuracy, F1 score, and top-k accuracy are then compared against those of earlier model versions.
```{r}
#TODO isnt this more part of monitoring?
```


</div>
</details>

# Model wrapping

- Encapsulates a trained and validated model for easy service
- While the model *per se* takes preprocessed/tokenized tensors as input, the wrapper aims at taking RAW text and outputting readable predictions (not logits)
    - A single `.predict()` method should work seamlessly
- Handle all the preprocessing steps (and the internal machinery needed to run inference)
- The package torchTextClassifiers has been developed in this mindset
- MLFlow is also naturally designed to help you do that

<details class="austria">
<summary class="austria-header">
  <span class="solutionbox-icon"></span>
  Austria: 
</summary>

<div class="solutionbox-body">

We implemented a single function that handles all pre- and post-processing steps required for the model. This function takes raw input data and transforms it into a format suitable for model inference. During pre-processing, text inputs are tokenized, and categorical variables—mostly in textual form—are translated into the integer values used internally by the model.

The model outputs a vector of probabilities, with one probability for each possible code per input. During post-processing, these probabilities are sorted to generate the top-k most likely codes.

</div>
</details>

# Model storage & versioning

- You should keep track of all the experiments (all the architectures, all the different hyperparameters), and you should be able to load any experiment that you have tried, at any time
- The logging tools generally also handles the storage part
- To "promote" a model once you are satisfied with its performance (and make it ready for deployment), you should have a way to tag and version your models (ex: SVM-v2, BERT-v4...) and so on.
    - At deployment time, you should be able to fetch a model only using its tag and its version (including a previous one if something suddenly broke !)

<details class="austria">
<summary class="austria-header">
  <span class="solutionbox-icon"></span>
  Austria: 
</summary>

<div class="solutionbox-body">

Model versions are stored locally. Once deployed to the rsconnect server, we can switch between model versions, enabling the reproduction of past experiments.

</div>
</details>